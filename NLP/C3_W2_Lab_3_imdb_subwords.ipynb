{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYTF8hFFQPXs"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W2/ungraded_labs/C3_W2_Lab_3_imdb_subwords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLKIel77CJPi"
   },
   "source": [
    "## Ungraded Lab: Subword Tokenization with the IMDB Reviews Dataset\n",
    "\n",
    "In this lab, you will look at a pre-tokenized dataset that is using subword text encoding. This is an alternative to word-based tokenization which you have been using in the previous labs. You will see how it works and its implications on preparing your data and training your model.\n",
    "\n",
    "Let's begin!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrzOn9quZ0Sv"
   },
   "source": [
    "## Download the IMDB reviews plain text and tokenized datasets\n",
    "\n",
    "First, you will download the [IMDB Reviews](https://www.tensorflow.org/datasets/catalog/imdb_reviews) dataset from Tensorflow Datasets. You will get two configurations:\n",
    "\n",
    "* `plain_text` - this is the default and the one you used in Lab 1 of this week\n",
    "* `subwords8k` - a pre-tokenized dataset (i.e. instead of sentences of type string, it will already give you the tokenized sequences). You will see how this looks in later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_IoM4VFxWpMR"
   },
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "Failed to construct dataset imdb_reviews: Message type \"tensorflow_datasets.DatasetInfo\" has no field named \"releaseNotes\".\n Available Fields(except extensions): ['name', 'description', 'version', 'configName', 'configDescription', 'citation', 'sizeInBytes', 'downloadSize', 'location', 'downloadChecksums', 'schema', 'splits', 'supervisedKeys', 'redistributionInfo', 'moduleName', 'disableShuffling', 'fileFormat']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParseError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mE:\\anaconda3\\envs\\tf\\lib\\site-packages\\google\\protobuf\\json_format.py:515\u001b[0m, in \u001b[0;36m_Parser._ConvertFieldValuePair\u001b[1;34m(self, js, message)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 515\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\n\u001b[0;32m    516\u001b[0m       (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMessage type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m has no field named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    517\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Available Fields(except extensions): \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    518\u001b[0m            message_descriptor\u001b[38;5;241m.\u001b[39mfull_name, name,\n\u001b[0;32m    519\u001b[0m            [f\u001b[38;5;241m.\u001b[39mjson_name \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m message_descriptor\u001b[38;5;241m.\u001b[39mfields]))\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names:\n",
      "\u001b[1;31mParseError\u001b[0m: Message type \"tensorflow_datasets.DatasetInfo\" has no field named \"releaseNotes\".\n Available Fields(except extensions): ['name', 'description', 'version', 'configName', 'configDescription', 'citation', 'sizeInBytes', 'downloadSize', 'location', 'downloadChecksums', 'schema', 'splits', 'supervisedKeys', 'redistributionInfo', 'moduleName', 'disableShuffling', 'fileFormat']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mParseError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Download the plain text default config\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m imdb_plaintext, info_plaintext \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimdb_reviews\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_supervised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Download the subword encoded pretokenized dataset\u001b[39;00m\n\u001b[0;32m      7\u001b[0m imdb_subwords, info_subwords \u001b[38;5;241m=\u001b[39m tfds\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb_reviews/subwords8k\u001b[39m\u001b[38;5;124m\"\u001b[39m, with_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, as_supervised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:315\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    313\u001b[0m   builder_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 315\u001b[0m dbuilder \u001b[38;5;241m=\u001b[39m builder(name, data_dir\u001b[38;5;241m=\u001b[39mdata_dir, try_gcs\u001b[38;5;241m=\u001b[39mtry_gcs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbuilder_kwargs)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m    317\u001b[0m   download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:166\u001b[0m, in \u001b[0;36mbuilder\u001b[1;34m(name, try_gcs, **builder_kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[0;32m    165\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m py_utils\u001b[38;5;241m.\u001b[39mtry_reraise(prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to construct dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbuilder_kwargs)  \u001b[38;5;66;03m# pytype: disable=not-instantiable\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# If neither the code nor the files are found, raise DatasetNotFoundError\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m not_found_error\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:900\u001b[0m, in \u001b[0;36mFileReaderBuilder.__init__\u001b[1;34m(self, file_format, **kwargs)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    887\u001b[0m              \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    888\u001b[0m              file_format: Union[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mstr\u001b[39m, file_adapters\u001b[38;5;241m.\u001b[39mFileFormat] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    889\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m    890\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Initializes an instance of FileReaderBuilder.\u001b[39;00m\n\u001b[0;32m    891\u001b[0m \n\u001b[0;32m    892\u001b[0m \u001b[38;5;124;03m  Callers must pass arguments as keyword arguments.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;124;03m    **kwargs: Arguments passed to `DatasetBuilder`.\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 900\u001b[0m   \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    901\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mset_file_format(file_format)\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:182\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[1;34m(self, data_dir, config, version)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_dir_root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_data_dir(data_dir)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_dir):\n\u001b[1;32m--> 182\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Use the code version (do not restore data)\u001b[39;00m\n\u001b[0;32m    184\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39minitialize_from_bucket()\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_info.py:369\u001b[0m, in \u001b[0;36mDatasetInfo.read_from_directory\u001b[1;34m(self, dataset_info_dir)\u001b[0m\n\u001b[0;32m    362\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    363\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry to load `DatasetInfo` from a directory which does not exist or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not contain `dataset_info.json`. Please delete the directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_info_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`  if you are trying to re-generate the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;66;03m# Load the metadata from disk\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m parsed_proto \u001b[38;5;241m=\u001b[39m \u001b[43mread_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# Update splits\u001b[39;00m\n\u001b[0;32m    372\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m splits_lib\u001b[38;5;241m.\u001b[39mSplitDict\u001b[38;5;241m.\u001b[39mfrom_proto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, parsed_proto\u001b[38;5;241m.\u001b[39msplits)\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_info.py:547\u001b[0m, in \u001b[0;36mread_from_json\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    545\u001b[0m json_str \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mas_path(path)\u001b[38;5;241m.\u001b[39mread_text()\n\u001b[0;32m    546\u001b[0m \u001b[38;5;66;03m# Parse it back into a proto.\u001b[39;00m\n\u001b[1;32m--> 547\u001b[0m parsed_proto \u001b[38;5;241m=\u001b[39m \u001b[43mjson_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_info_pb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDatasetInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parsed_proto\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\tf\\lib\\site-packages\\google\\protobuf\\json_format.py:425\u001b[0m, in \u001b[0;36mParse\u001b[1;34m(text, message, ignore_unknown_fields, descriptor_pool)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    424\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to load JSON: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(e)))\n\u001b[1;32m--> 425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParseDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_unknown_fields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescriptor_pool\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\tf\\lib\\site-packages\\google\\protobuf\\json_format.py:445\u001b[0m, in \u001b[0;36mParseDict\u001b[1;34m(js_dict, message, ignore_unknown_fields, descriptor_pool)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parses a JSON dictionary representation into a message.\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;124;03m  The same message passed as argument.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    444\u001b[0m parser \u001b[38;5;241m=\u001b[39m _Parser(ignore_unknown_fields, descriptor_pool)\n\u001b[1;32m--> 445\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConvertMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjs_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\tf\\lib\\site-packages\\google\\protobuf\\json_format.py:476\u001b[0m, in \u001b[0;36m_Parser.ConvertMessage\u001b[1;34m(self, value, message)\u001b[0m\n\u001b[0;32m    474\u001b[0m   methodcaller(_WKTJSONMETHODS[full_name][\u001b[38;5;241m1\u001b[39m], value, message)(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 476\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ConvertFieldValuePair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\tf\\lib\\site-packages\\google\\protobuf\\json_format.py:590\u001b[0m, in \u001b[0;36m_Parser._ConvertFieldValuePair\u001b[1;34m(self, js, message)\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to parse \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m field: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, e))\n\u001b[0;32m    589\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 590\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    592\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to parse \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m field: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, e))\n",
      "\u001b[1;31mParseError\u001b[0m: Failed to construct dataset imdb_reviews: Message type \"tensorflow_datasets.DatasetInfo\" has no field named \"releaseNotes\".\n Available Fields(except extensions): ['name', 'description', 'version', 'configName', 'configDescription', 'citation', 'sizeInBytes', 'downloadSize', 'location', 'downloadChecksums', 'schema', 'splits', 'supervisedKeys', 'redistributionInfo', 'moduleName', 'disableShuffling', 'fileFormat']"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Download the plain text default config\n",
    "imdb_plaintext, info_plaintext = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "\n",
    "# Download the subword encoded pretokenized dataset\n",
    "imdb_subwords, info_subwords = tfds.load(\"imdb_reviews/subwords8k\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JggMZRCEcdlN"
   },
   "source": [
    "## Compare the two datasets\n",
    "\n",
    "As mentioned, the data types returned by the two datasets will be different. For the default, it will be strings as you also saw in Lab 1. Notice the description of the `text` key below and the sample sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3J7IAJMGH-VN"
   },
   "outputs": [],
   "source": [
    "# Print description of features\n",
    "info_plaintext.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTO45ghxc4js"
   },
   "outputs": [],
   "source": [
    "# Take 2 training examples and print the text feature\n",
    "for example in imdb_plaintext['train'].take(2):\n",
    "  print(example[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f87JvGD9dId5"
   },
   "source": [
    "For `subwords8k`, the dataset is already tokenized so the data type will be integers. Notice that the `text` features also include an `encoder` field and has a `vocab_size` of around 8k, hence the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wp_a7292mxk"
   },
   "outputs": [],
   "source": [
    "# Print description of features\n",
    "info_subwords.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ssDU_TddyLF"
   },
   "source": [
    "If you print the results, you will not see string sentences but a sequence of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35oQQIUG21cG"
   },
   "outputs": [],
   "source": [
    "# Take 2 training examples and print its contents\n",
    "for example in imdb_subwords['train'].take(2):\n",
    "  print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWOrkYGug--B"
   },
   "source": [
    "You can get the `encoder` object included in the download and use it to decode the sequences above. You'll see that you will arrive at the same sentences provided in the `plain_text` config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kNEGgEgfO6x"
   },
   "outputs": [],
   "source": [
    "# Get the encoder\n",
    "tokenizer_subwords = info_subwords.features['text'].encoder\n",
    "\n",
    "# Take 2 training examples and decode the text feature\n",
    "for example in imdb_subwords['train'].take(2):\n",
    "  print(tokenizer_subwords.decode(example[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20_XNWbXiwcE"
   },
   "source": [
    "*Note: The documentation for the encoder can be found [here](https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/SubwordTextEncoder) but don't worry if it's marked as deprecated. As mentioned, the objective of this exercise is just to show the characteristics of subword encoding.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKrbY2fjjFHM"
   },
   "source": [
    "## Subword Text Encoding\n",
    "\n",
    "From previous labs, the number of tokens in the sequence is the same as the number of words in the text (i.e. word tokenization). The following cells shows a review of this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6ly_yOIkM-K"
   },
   "outputs": [],
   "source": [
    "# Get the train set\n",
    "train_data = imdb_plaintext['train']\n",
    "\n",
    "# Initialize sentences list\n",
    "training_sentences = []\n",
    "\n",
    "# Loop over all training examples and save to the list\n",
    "for s,_ in train_data:\n",
    "  training_sentences.append(s.numpy().decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-N6Yd_TE3gZ5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 10000\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "# Initialize the Tokenizer class\n",
    "tokenizer_plaintext = Tokenizer(num_words = 10000, oov_token=oov_tok)\n",
    "\n",
    "# Generate the word index dictionary for the training sentences\n",
    "tokenizer_plaintext.fit_on_texts(training_sentences)\n",
    "\n",
    "# Generate the training sequences\n",
    "sequences = tokenizer_plaintext.texts_to_sequences(training_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNUlDp76lf94"
   },
   "source": [
    "The cell above uses a `vocab_size` of 10000 but you'll find that it's easy to find OOV tokens when decoding using the lookup dictionary it created. See the result below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmsECyVr4OPE"
   },
   "outputs": [],
   "source": [
    "# Decode the first sequence using the Tokenizer class\n",
    "tokenizer_plaintext.sequences_to_texts(sequences[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0HQqkBmpujb"
   },
   "source": [
    "For binary classifiers, this might not have a big impact but you may have other applications that will benefit from avoiding OOV tokens when training the model (e.g. text generation). If you want the tokenizer above to not have OOVs, then the `vocab_size` will increase to more than 88k. This can slow down training and bloat the model size. The encoder also won't be robust when used on other datasets which may contain new words, thus resulting in OOVs again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7m-Ds9lpUQc"
   },
   "outputs": [],
   "source": [
    "# Total number of words in the word index dictionary\n",
    "len(tokenizer_plaintext.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McxNKhHIsNvl"
   },
   "source": [
    "*Subword text encoding* gets around this problem by using parts of the word to compose whole words. This makes it more flexible when it encounters uncommon words. See how these subwords look like for this particular encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqyMSZbnwFBo"
   },
   "outputs": [],
   "source": [
    "# Print the subwords\n",
    "print(tokenizer_subwords.subwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaRA9LBUwfHM"
   },
   "source": [
    "If you use it on the previous plain text sentence, you'll see that it won't have any OOVs even if it has a smaller vocab size (only 8k compared to 10k above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tn_eLaS5mR7H"
   },
   "outputs": [],
   "source": [
    "# Encode the first plaintext sentence using the subword text encoder\n",
    "tokenized_string = tokenizer_subwords.encode(training_sentences[0])\n",
    "print(tokenized_string)\n",
    "\n",
    "# Decode the sequence\n",
    "original_string = tokenizer_subwords.decode(tokenized_string)\n",
    "\n",
    "# Print the result\n",
    "print (original_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iL9O3hEqw4Bl"
   },
   "source": [
    "Subword encoding can even perform well on words that are not commonly found on movie reviews. See first the result when using the plain text tokenizer. As expected, it will show many OOVs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHRj1J0j8ApE"
   },
   "outputs": [],
   "source": [
    "# Define sample sentence\n",
    "sample_string = 'TensorFlow, from basics to mastery'\n",
    "\n",
    "# Encode using the plain text tokenizer\n",
    "tokenized_string = tokenizer_plaintext.texts_to_sequences([sample_string])\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "# Decode and print the result\n",
    "original_string = tokenizer_plaintext.sequences_to_texts(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhQ-4O-uxdbJ"
   },
   "source": [
    "Then compare to the subword text encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fPl2BXhYEHRP"
   },
   "outputs": [],
   "source": [
    "# Encode using the subword text encoder\n",
    "tokenized_string = tokenizer_subwords.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "# Decode and print the results\n",
    "original_string = tokenizer_subwords.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89sbfXjz0MSW"
   },
   "source": [
    "As you may notice, the sentence is correctly decoded. The downside is the token sequence is much longer. Instead of only 5 when using word-encoding, you ended up with 11 tokens instead. The mapping for this sentence is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3t7vvNLEZml"
   },
   "outputs": [],
   "source": [
    "# Show token to subword mapping:\n",
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer_subwords.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ22ugch1TFy"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "You will now train your model using this pre-tokenized dataset. Since these are already saved as sequences, you can jump straight to making uniform sized arrays for the train and test sets. These are also saved as `tf.data.Dataset` type so you can use the [`padded_batch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch) method to create batches and pad the arrays into a uniform size for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVSTLBe_SOUr"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Get the train and test splits\n",
    "train_data, test_data = imdb_subwords['train'], imdb_subwords['test'],\n",
    "\n",
    "# Shuffle the training data\n",
    "train_dataset = train_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# Batch and pad the datasets to the maximum length of the sequences\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
    "test_dataset = test_data.padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCjHCG7s2sAR"
   },
   "source": [
    "Next, you will build the model. You can just use the architecture from the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NEpdhb8AxID"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define dimensionality of the embedding\n",
    "embedding_dim = 64\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer_subwords.vocab_size, embedding_dim),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aOn2bAc3AUj"
   },
   "source": [
    "Similarly, you can use the same parameters for training. In Colab, it will take around 20 seconds per epoch (without an accelerator) and you will reach around 94% training accuracy and 88% validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkt8c5dNuUlT"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Set the training parameters\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Start training\n",
    "history = model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ygYaD6H3qGX"
   },
   "source": [
    "## Visualize the results\n",
    "\n",
    "You can use the cell below to plot the training results. See if you can improve it by tweaking the parameters such as the size of the embedding and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_rMnm7WxQGT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot utility\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "# Plot the accuracy and results\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0TRE-Lb4C5b"
   },
   "source": [
    "## Wrap Up\n",
    "\n",
    "In this lab, you saw how subword text encoding can be a robust technique to avoid out-of-vocabulary tokens. It can decode uncommon words it hasn't seen before even with a relatively small vocab size. Consequently, it results in longer token sequences when compared to full word tokenization. Next week, you will look at other architectures that you can use when building your classifier. These will be recurrent neural networks and convolutional neural networks."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "C3_W2_Lab_3_imdb_subwords.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
