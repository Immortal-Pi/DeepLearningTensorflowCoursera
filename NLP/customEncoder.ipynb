{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1dd40ae-9768-46b3-9fb3-c8cee527c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d67de920-0d4c-46dc-9e60-4cf37128c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 100  # Example input dimension\n",
    "encoding_dim = 32  # Dimension of the encoded representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34733e07-4c5a-4537-af1a-e55eb86b1074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 100) for input KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='input_3'), name='input_3', description=\"created by layer 'input_3'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"E:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"E:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"E:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"E:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"E:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"E:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"model_2\" \"                 f\"(type Functional).\n    \n    Input 0 of layer \"dense_2\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer \"model_2\" \"                 f\"(type Functional):\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Fit the tokenizer on your text data\u001b[39;00m\n\u001b[0;32m     27\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample text data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manother example text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 28\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_on_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\preprocessing\\text.py:300\u001b[0m, in \u001b[0;36mTokenizer.fit_on_texts\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    293\u001b[0m         seq \u001b[38;5;241m=\u001b[39m text_to_word_sequence(\n\u001b[0;32m    294\u001b[0m             text,\n\u001b[0;32m    295\u001b[0m             filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters,\n\u001b[0;32m    296\u001b[0m             lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlower,\n\u001b[0;32m    297\u001b[0m             split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit,\n\u001b[0;32m    298\u001b[0m         )\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 300\u001b[0m         seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m seq:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_counts:\n",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m, in \u001b[0;36mcustom_analyzer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/encoder.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     15\u001b[0m     encoder1\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m---> 16\u001b[0m encoded_seq \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Flatten the encoded vectors to a single list (optional, depends on your use case)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m encoded_words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(word_vector) \u001b[38;5;28;01mfor\u001b[39;00m word_vector \u001b[38;5;129;01min\u001b[39;00m encoded_seq]\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\ANONYM~1\\AppData\\Local\\Temp\\__autograph_generated_fileyyyx2s3v.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"E:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"E:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"E:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"E:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"E:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"E:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"model_2\" \"                 f\"(type Functional).\n    \n    Input 0 of layer \"dense_2\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer \"model_2\" \"                 f\"(type Functional):\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "# Define the custom analyzer function\n",
    "def custom_analyzer(text):\n",
    "    # Preprocess text (e.g., convert to lower case, remove punctuation, etc.)\n",
    "    seq = text_to_word_sequence(text)\n",
    "\n",
    "    # Example: Encode the word sequences (here, just a placeholder example)\n",
    "    # Convert words to their corresponding encoded representation\n",
    "    # This is an example and may need to be adapted to your encoder's input requirements\n",
    "    # dummy_input = np.random.rand(len(seq), input_dim)  # Replace with actual input processing\n",
    "    with open('datasets/encoder.pkl','rb') as file:\n",
    "        encoder1=pickle.load(file)\n",
    "    encoded_seq = encoder.predict(encoder1)\n",
    "\n",
    "    # Flatten the encoded vectors to a single list (optional, depends on your use case)\n",
    "    encoded_words = [str(word_vector) for word_vector in encoded_seq]\n",
    "\n",
    "    return encoded_words\n",
    "\n",
    "# Create the tokenizer with the custom analyzer\n",
    "tokenizer = Tokenizer(num_words=1000, analyzer=custom_analyzer)\n",
    "\n",
    "# Fit the tokenizer on your text data\n",
    "texts = [\"sample text data\", \"another example text\"]\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf354c28-1c67-4bd0-a63f-54e7c3351335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[0.20980585 0.6557867  0.68001163 0.         0.9924443  0.\\n 0.75006354 0.         0.         0.         0.3705008  1.2068629\\n 0.47504577 0.61509156 0.06299833 0.         0.14243451 1.7373215\\n 0.         0.14127892 0.         0.787348   0.26273313 0.\\n 0.         0.46006092 0.         0.         0.9039314  0.01102431\\n 0.         1.1089686 ]': 1,\n",
       " '[0.48583388 0.         0.98349696 0.         1.0458555  0.\\n 1.1680969  0.         0.         0.         0.8799448  1.5096087\\n 0.3862797  0.84041905 0.34319103 0.10999287 0.0277928  1.8869615\\n 0.         0.         0.         0.94568527 0.7807915  0.\\n 0.         0.         0.         0.5842867  0.         0.26701722\\n 0.26609087 1.181139  ]': 2,\n",
       " '[0.48698163 0.37929696 0.5792938  0.         0.11867303 0.4610809\\n 1.2221005  0.         0.         0.         0.21899939 1.2353173\\n 0.00812255 1.0487175  0.08911172 0.         0.         1.2073157\\n 0.         0.         0.         0.44966617 0.9006368  0.7686726\\n 0.         0.1014259  0.14610061 0.         0.         0.9801713\\n 0.         0.59451306]': 3,\n",
       " '[0.6222713  0.87221724 0.         0.         1.2364792  0.\\n 0.7060468  0.         0.         0.         0.5994261  1.3025849\\n 0.3204106  1.0248822  0.32390484 0.23169151 0.         1.5669264\\n 0.         0.         0.         0.031482   0.43874794 0.33032948\\n 0.         0.         0.72929144 0.         0.         0.72835046\\n 0.         0.9547736 ]': 4,\n",
       " '[1.2016286  0.5050657  0.64111316 0.         1.4797914  0.\\n 1.064848   0.         0.         0.24756192 1.1078773  1.5374355\\n 1.0836859  0.64495444 0.         0.         0.         1.2381873\\n 0.         0.06695232 0.         1.0704458  0.03912294 0.\\n 0.         0.         0.11177433 0.         0.3846023  0.564715\\n 0.         0.5207941 ]': 5,\n",
       " '[0.20425068 0.         1.15663    0.         0.5418629  0.33793575\\n 0.97638416 0.         0.         0.         0.27218932 1.6374307\\n 0.9238555  0.8313387  0.         0.3396938  0.         1.3166146\\n 0.         0.         0.         0.29309863 0.61572224 0.7185693\\n 0.         0.30071276 0.         0.30691308 0.74157846 0.28482437\\n 0.         1.1047535 ]': 6}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fcaac1-3d48-4b83-ad95-5355525f2ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
